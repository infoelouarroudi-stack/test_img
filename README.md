# DeepAnimDance ‚Äî Posture‚ÄëGuided Person Image Synthesis (TP "Everybody Dance Now")

*(README au format rapport comme requis : pas de rapport PDF s√©par√©)*

---

## üìë Table des mati√®res

- [1) Structure du d√©p√¥t (projet principal)](#1-structure-du-d√©p√¥t-projet-principal)
- [2) Environnements (venv + conda)](#2-environnements-venv--conda)
  - [Option A ‚Äî venv (pip)](#option-a--venv-pip)
  - [Option B ‚Äî conda (environment.yml)](#option-b--conda-environmentyml)
- [3) Ex√©cuter la d√©mo (projet principal)](#3-ex√©cuter-la-d√©mo-projet-principal)
  - [3.1 Important : ex√©cuter depuis la racine du d√©p√¥t + PYTHONPATH](#31-important--ex√©cuter-depuis-la-racine-du-d√©p√¥t--pythonpath)
  - [3.2 Ce que la d√©mo affiche](#32-ce-que-la-d√©mo-affiche)
  - [3.3 Choisir le g√©n√©rateur (GEN_TYPE)](#33-choisir-le-g√©n√©rateur-gen_type)
- [4) Aper√ßu du pipeline (donn√©es ‚Üí squelette ‚Üí g√©n√©ration)](#4-aper√ßu-du-pipeline-donn√©es--squelette--g√©n√©ration)
  - [4.1 Construction et mise en cache de l'ensemble de donn√©es cible (VideoSkeleton)](#41-construction-et-mise-en-cache-de-lensemble-de-donn√©es-cible-videoskeleton)
  - [4.2 Extraction de squelette (MediaPipe Pose) et repr√©sentation](#42-extraction-de-squelette-mediapipe-pose-et-repr√©sentation)
  - [4.3 Recadrage et cas d'√©chec (robustesse de la d√©mo)](#43-recadrage-et-cas-d√©chec-robustesse-de-la-d√©mo)
- [5) M√©thodes et concepts (√©tapes TP)](#5-m√©thodes-et-concepts-√©tapes-tp)
  - [5.1 √âtape 1 ‚Äî Baseline Nearest Neighbor (GenNeirest)](#51-√©tape-1--baseline-nearest-neighbor-genneirest)
  - [5.2 √âtape 2 ‚Äî Vanilla NN (vecteur 26D ‚Üí image)](#52-√©tape-2--vanilla-nn-vecteur-26d--image--gennnske26toimage)
  - [5.3 √âtape 3 ‚Äî Vanilla NN (image stickman ‚Üí image)](#53-√©tape-3--vanilla-nn-image-stickman--image--gennnskeimltoimage)
  - [5.4 √âtape 4 ‚Äî Raffinement GAN](#54-√©tape-4--raffinement-gan--gengan-wgangp--l1)
- [6) Entra√Ænement (reproductibilit√©)](#6-entra√Ænement-reproductibilit√©)
  - [6.1 Optionnel : reconstruire le cache](#61-optionnel--reconstruire-le-cache)
  - [6.2 Entra√Æner VanillaNN (26D ou stickman)](#62-entra√Æner-vanillaNN-26d-ou-stickman)
  - [6.3 Entra√Æner GAN](#63-entra√Æner-gan)
- [7) Vid√©o de d√©monstration](#7-vid√©o-de-d√©monstration)
- [8) Bonus ‚Äî Application web Flask](#9-bonus--application-web-flask-ex√©cution-uniquement)
  - [8.1 R√¥le (ce qu'elle ajoute)](#91-r√¥le-ce-quelle-ajoute)
  - [8.2 Installation & ex√©cution (venv)](#92-installation--ex√©cution-venv)

---
## 1) Structure du d√©p√¥t (projet principal)

Structure typique (racine) :
- `src/` : code source (d√©mo, construction de l'ensemble de donn√©es, classe squelette, g√©n√©rateurs).
- `data/` : vid√©os + cache + poids entra√Æn√©s.
  - `data/taichi1.mp4` : vid√©o **cible** utilis√©e pour construire l'ensemble de donn√©es.
  - `data/taichi1.pkl` + `data/taichi1/` : squelettes/images en cache produits par `VideoSkeleton`.
  - `data/Dance/` : r√©seaux entra√Æn√©s (`.pth`) charg√©s par les g√©n√©rateurs.

---

## 2) Environnements (venv + conda)

Deux configurations ont √©t√© utilis√©es selon les membres de l'√©quipe :
- Un membre a utilis√© `venv` (pip).
- Un autre membre a utilis√© `conda` via `environment.yml`.

### Option A ‚Äî venv (pip)

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

pip install -r requirements.txt
```

### Option B ‚Äî conda (environment.yml)

```bash
conda env create -f environment.yml
conda activate <ENV_NAME>
```

---

## 3) Ex√©cuter la d√©mo (projet principal)

### 3.1 Important : ex√©cuter depuis la racine du d√©p√¥t + PYTHONPATH

Les chemins vers les poids sont cod√©s en dur comme des chemins relatifs comme `data/Dance/...`, et les imports sont √©crits comme `from VideoSkeleton import ...`, donc ex√©cutez depuis la racine du d√©p√¥t avec `src/` dans `PYTHONPATH`.

**Linux / macOS**

```bash
PYTHONPATH=src python src/DanceDemo.py
```

**Windows (PowerShell)**

```powershell
$env:PYTHONPATH="src"
python src\DanceDemo.py
```

### 3.2 Ce que la d√©mo affiche

`DanceDemo.py` affiche 3 panneaux : **VID√âO SOURCE | SQUELETTE | G√âN√âRATION**, et un overlay FPS.

Contr√¥les : `q` quitte, `n` saute ~100 images.

### 3.3 Choisir le g√©n√©rateur (GEN_TYPE)

Dans `DanceDemo.py`, s√©lectionnez le g√©n√©rateur avec `GEN_TYPE` (et une vid√©o source comme `data/taichi2.mp4`).

| GEN_TYPE | M√©thode | Description |
|---:|---|---|
| 1 | Nearest Neighbor | Baseline `GenNeirest` (pas d'apprentissage). |
| 2 | Vanilla NN (26D) | Vecteur de squelette r√©duit (26D) ‚Üí image. |
| 3 | Vanilla NN (stickman) | Image stickman ‚Üí image (encodeur‚Äëd√©codeur + skips). |
| 4 | GAN | WGAN‚ÄëGP + L1 (stickman ‚Üí image). |

---

## 4) Aper√ßu du pipeline (donn√©es ‚Üí squelette ‚Üí g√©n√©ration)

### 4.1 Construction et mise en cache de l'ensemble de donn√©es cible (VideoSkeleton)

`VideoSkeleton` construit un ensemble de donn√©es cible de paires (squelette, image) √† partir de la vid√©o cible.

Pour acc√©l√©rer les ex√©cutions r√©p√©t√©es, il utilise la mise en cache : il stocke les squelettes et les m√©tadonn√©es dans un `.pkl` et stocke les images extraites sur le disque ; les ex√©cutions suivantes rechargent les donn√©es en cache si disponibles.

### 4.2 Extraction de squelette (MediaPipe Pose) et repr√©sentation

Les squelettes sont extraits avec MediaPipe Pose (33 points de rep√®re avec x, y, z).

Pour l'apprentissage, un **squelette r√©duit** est utilis√© : 13 articulations avec des coordonn√©es (x, y) ‚Üí 26 valeurs (entr√©e de dimension inf√©rieure, apprentissage plus facile).

### 4.3 Recadrage et cas d'√©chec (robustesse de la d√©mo)

Pour chaque image source, la d√©mo utilise la logique de recadrage de l'ensemble de donn√©es cible (`cropAndSke`) pour recadrer autour de la pose d√©tect√©e avant de dessiner/g√©n√©rer.

Si aucun squelette n'est d√©tect√©, la d√©mo affiche un panneau d'erreur rouge et saute la g√©n√©ration pour cette image.

Pour maintenir une vitesse en temps r√©el, la d√©mo calcule une image sur 5 par d√©faut (modifiable).

---

## 5) M√©thodes et concepts (√©tapes TP)

Ce projet suit une approche progressive : baseline ‚Üí apprentissage supervis√© ‚Üí image‚Äë√†‚Äëimage ‚Üí raffinement GAN pour le r√©alisme.

### 5.1 √âtape 1 ‚Äî Baseline Nearest Neighbor (GenNeirest)

**Concept :** pour chaque pose source, trouver la pose la plus proche dans l'ensemble de donn√©es cible et sortir l'image cible r√©elle correspondante (pas de synth√®se).

**Impl√©mentation (votre code) :**
- It√©rer sur les squelettes cibles, calculer `ske.distance(target_ske)` et s√©lectionner le minimum.
- Retourner l'image cible correspondante convertie en float dans [0,1] pour compatibilit√© d'affichage.

**Limitations attendues :** mouvement saccad√© (pas de mod√®le temporel), limit√© aux poses existant dans l'ensemble de donn√©es cible, et recherche lin√©aire lente pour les grands ensembles de donn√©es.

### 5.2 √âtape 2 ‚Äî Vanilla NN (vecteur 26D ‚Üí image) ‚Äî `GenNNSke26ToImage`

**Concept :** apprendre une correspondance directe d'un vecteur de pose de 26 dimensions vers une image RGB cible 64√ó64.

**Architecture (votre code final) :**
- `Linear(26 ‚Üí 256*4*4)` puis reshape en un tenseur 4√ó4√ó256.
- Sur√©chantillonnage avec des blocs `ConvTranspose2d` pour atteindre 64√ó64.
- Stabilisation/qualit√© : `BatchNorm2d`, `LeakyReLU`, et plusieurs `ResidualBlock`s.
- Sortie : `Tanh()` ‚Üí valeurs dans [-1,1] (align√©es avec la normalisation cible).

**Entra√Ænement :** `GenVanillaNN.train()` utilise Adam et `MSELoss` (r√©gression pixel par pixel), qui est simple mais conduit souvent au flou (effet de moyenne).

### 5.3 √âtape 3 ‚Äî Vanilla NN (image stickman ‚Üí image) ‚Äî `GenNNSkeImToImage`

**Concept :** convertir le squelette en une "image stickman" interm√©diaire et r√©soudre la traduction image-√†-image.

**Cr√©ation du stickman :** `SkeToImageTransform(64)` cr√©e une image noire, dessine le squelette (dessin BGR color√©), puis convertit en RGB avant `ToTensor()`.

**Architecture (votre code final) :**
- Encodeur : 4 couches de sous-√©chantillonnage `Conv2d` (64√ó64 ‚Üí 4√ó4) pour extraire les caract√©ristiques de pose.
- Module ajout√© : `SelfAttention(256)` au niveau de caract√©ristiques 8√ó8√ó256 pour capturer les d√©pendances √† longue port√©e.
- Goulot d'√©tranglement : blocs r√©siduels empil√©s (`ResidualBlock(512)` r√©p√©t√©s).
- D√©codeur : sur√©chantillonnage `ConvTranspose2d` pour reconstruire 64√ó64.
- Connexions r√©siduelles : concat√©nations explicites `torch.cat([...])` (style U‚ÄëNet).
- Sortie : `Tanh()` dans [-1,1].

**Entra√Ænement :** toujours supervis√© avec `MSELoss`, donc la pose est respect√©e mais les textures peuvent rester lisses.

### 5.4 √âtape 4 ‚Äî Raffinement GAN ‚Äî `GenGAN` (WGAN‚ÄëGP + L1)

**Motivation :** les pertes de pixels supervis√©es (en particulier MSE) produisent souvent des images floues ; l'entra√Ænement GAN encourage des textures plus nettes et des sorties plus r√©alistes.

**Discriminateur/Critique :** CNN de style PatchGAN produisant une carte de patchs, sans sigmo√Øde (score du critique WGAN).

**Pertes impl√©ment√©es (votre code) :**
- Critique (WGAN‚ÄëGP) : D(fake).mean - D(real).mean + Œª_gp*GP avec p√©nalit√© de gradient calcul√©e sur des √©chantillons interpol√©s.
- G√©n√©rateur : terme adversarial -D(fake).mean + terme de reconstruction `lambda_l1 * L1(fake, real)` avec `lambda_l1 = 100`.

**D√©tails de la boucle d'entra√Ænement (votre code) :**
- `n_critic = 5` mises √† jour du critique par mise √† jour du g√©n√©rateur.
- Adam avec `betas=(0.0, 0.9)`, `lr_g=1e-4`, `lr_d=4e-4`.
- Point de contr√¥le enregistr√© comme `{"netG": state_dict, "netD": state_dict}` dans `data/Dance/DanceGenGAN.pth`.

---

## 6) Entra√Ænement (reproductibilit√©)

L'entra√Ænement utilise l'ensemble de donn√©es cible construit √† partir de `data/taichi1.mp4`.

### 6.1 Optionnel : reconstruire le cache

Si n√©cessaire, supprimez le cache et le r√©pertoire d'images puis r√©ex√©cutez l'entra√Ænement/d√©mo pour que `VideoSkeleton` les recalcule :

```bash
rm -f data/taichi1.pkl
rm -rf data/taichi1/
```

(Les utilisateurs Windows peuvent supprimer manuellement depuis l'Explorateur de fichiers.)

### 6.2 Entra√Æner VanillaNN (26D ou stickman)

Dans `GenVanillaNN.py`, configurez :
- `optSkeOrImage = 1` pour vecteur(26D)‚Üíimage
- `optSkeOrImage = 2` pour stickman‚Üíimage

et activez l'entra√Ænement dans la section `__main__`.

Ex√©cutez :

```bash
PYTHONPATH=src python src/GenVanillaNN.py data/taichi1.mp4
```

Poids enregistr√©s (selon le mode) :
- `data/Dance/DanceGenVanillaFromSke26.pth`
- `data/Dance/DanceGenVanillaFromSkeim.pth`

### 6.3 Entra√Æner GAN

Dans `GenGAN.py`, d√©finissez `train = True` et ex√©cutez :

```bash
PYTHONPATH=src python src/GenGAN.py data/taichi1.mp4
```

Sortie :
- `data/Dance/DanceGenGAN.pth`

---

## 7) Vid√©o de d√©monstration

Une vid√©o de d√©monstration d'environ 2 minutes est fournie dans ce d√©p√¥t :

- `demo.mp4` (ou lien) : <PUT_LINK_HERE>

La vid√©o montre :
- L'ex√©cution de `src/DanceDemo.py` depuis la racine du d√©p√¥t avec les r√©seaux entra√Æn√©s.
- Le passage entre au moins deux modes (par exemple, `GEN_TYPE=1` baseline et `GEN_TYPE=4` GAN).
- Les 3 panneaux (SOURCE | SQUELETTE | G√âN√âRATION), l'affichage FPS, et la sortie avec `q`.

---

## 9) Bonus ‚Äî Application web Flask (ex√©cution uniquement)

D√©p√¥t GitHub : https://github.com/infoelouarroudi-stack/DemoDaanceWEB

### 9.1 R√¥le (ce qu'elle ajoute)

Une petite interface Flask a √©t√© ajout√©e en **bonus** pour ex√©cuter/visualiser le projet depuis un navigateur (wrapper UI), tout en gardant le projet principal comme c≈ìur not√©.

Structure :
- `app.py` : point d'entr√©e du serveur Flask.
- `templates/index.html` : page d'accueil.
- `templates/viewer.html` : page de visualisation des r√©sultats.
- `static/` : CSS/JS/assets.

### 9.2 Installation & ex√©cution (venv)

Depuis la racine du d√©p√¥t de l'application web (o√π se trouvent `app.py` et `requirements.txt`) :

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

pip install -r requirements.txt
python app.py
```

Puis ouvrez (serveur de d√©veloppement Flask par d√©faut) : http://localhost:5000/

---
