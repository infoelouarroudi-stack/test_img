# DeepAnimDance ‚Äî Posture-guided Person Image Synthesis (TP "Everybody Dance Now")

> README en format "mini-rapport" (√† la place d'un rapport PDF), demand√© pour l'√©valuation.

---

## üìë Table des mati√®res

- [Contexte & objectif](#contexte--objectif)
  - [Livrables (√©valuation)](#livrables-√©valuation)
- [1) Structure du d√©p√¥t (projet principal)](#1-structure-du-d√©p√¥t-projet-principal)
  - [Arborescence (exemple)](#arborescence-exemple)
  - [R√¥le de chaque fichier (src/)](#r√¥le-de-chaque-fichier-src)
- [2) Environnements (venv + conda)](#2-environnements-venv--conda)
  - [Option A ‚Äî venv (pip)](#option-a--venv-pip)
  - [Option B ‚Äî conda (environment.yml)](#option-b--conda-environmentyml)
- [3) Ex√©cuter la d√©mo (projet principal)](#3-ex√©cuter-la-d√©mo-projet-principal)
  - [3.1 Important : ex√©cuter depuis la racine du d√©p√¥t + PYTHONPATH](#31-important--ex√©cuter-depuis-la-racine-du-d√©p√¥t--pythonpath)
  - [3.2 Ce que la d√©mo affiche](#32-ce-que-la-d√©mo-affiche)
  - [3.3 Choisir le g√©n√©rateur (GEN_TYPE)](#33-choisir-le-g√©n√©rateur-gen_type)
- [4) Aper√ßu du pipeline (donn√©es ‚Üí squelette ‚Üí g√©n√©ration)](#4-aper√ßu-du-pipeline-donn√©es--squelette--g√©n√©ration)
  - [4.1 Construction et mise en cache de l'ensemble de donn√©es cible (VideoSkeleton)](#41-construction-et-mise-en-cache-de-lensemble-de-donn√©es-cible-videoskeleton)
  - [4.2 Extraction de squelette (MediaPipe Pose) et repr√©sentation](#42-extraction-de-squelette-mediapipe-pose-et-repr√©sentation)
  - [4.3 Recadrage et cas d'√©chec (robustesse de la d√©mo)](#43-recadrage-et-cas-d√©chec-robustesse-de-la-d√©mo)
- [5) M√©thodes et concepts (√©tapes TP)](#5-m√©thodes-et-concepts-√©tapes-tp)
  - [5.1 √âtape 1 ‚Äî Baseline Nearest Neighbor (GenNeirest)](#51-√©tape-1--baseline-nearest-neighbor-genneirest)
  - [5.2 √âtape 2 ‚Äî Vanilla NN (vecteur 26D ‚Üí image)](#52-√©tape-2--vanilla-nn-vecteur-26d--image--gennnske26toimage)
  - [5.3 √âtape 3 ‚Äî Vanilla NN (image stickman ‚Üí image)](#53-√©tape-3--vanilla-nn-image-stickman--image--gennnskeimtoimage)
  - [5.4 √âtape 4 ‚Äî Raffinement GAN](#54-√©tape-4--raffinement-gan--gengan-wgangp--l1)
- [6) Entra√Ænement (reproductibilit√©)](#6-entra√Ænement-reproductibilit√©)
  - [6.1 Optionnel : reconstruire le cache](#61-optionnel--reconstruire-le-cache)
  - [6.2 Entra√Æner VanillaNN (26D ou stickman)](#62-entra√Æner-vanillann-26d-ou-stickman)
  - [6.3 Entra√Æner GAN](#63-entra√Æner-gan)
- [7) Vid√©o de d√©monstration](#7-vid√©o-de-d√©monstration)
- [8) Bonus ‚Äî Application web Flask (ex√©cution uniquement)](#8-bonus--application-web-flask-ex√©cution-uniquement)
  - [8.1 R√¥le (ce qu'elle ajoute)](#81-r√¥le-ce-quelle-ajoute)
  - [8.2 Installation & ex√©cution (venv)](#82-installation--ex√©cution-venv)
- [Limitations (observ√©es / attendues)](#limitations-observ√©es--attendues)
- [Cr√©dits](#cr√©dits)

---

## Contexte & objectif

Ce projet impl√©mente la **synth√®se d'images guid√©e par la pose** ("posture-guided image synthesis") : on transf√®re le mouvement (s√©quence de poses) d'une **vid√©o source** vers une **identit√© cible** apprise √† partir d'une vid√©o cible.

Le pipeline global est : **Vid√©o ‚Üí Extraction squelette (MediaPipe) ‚Üí G√©n√©ration d'image (plusieurs m√©thodes) ‚Üí Affichage/D√©mo**.

### Livrables (√©valuation)

- Un ZIP incluant : tout le code, les donn√©es n√©cessaires, les poids entra√Æn√©s (`.pth`) et une vid√©o de d√©monstration (~2 min).
- Pas de rapport s√©par√© : ce README contient les explications techniques.

---

## 1) Structure du d√©p√¥t (projet principal)

### Arborescence (exemple)

```
project-root/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ DanceDemo.py
‚îÇ   ‚îú‚îÄ‚îÄ VideoSkeleton.py
‚îÇ   ‚îú‚îÄ‚îÄ VideoReader.py
‚îÇ   ‚îú‚îÄ‚îÄ Skeleton.py
‚îÇ   ‚îú‚îÄ‚îÄ Vec3.py
‚îÇ   ‚îú‚îÄ‚îÄ GenNearest.py
‚îÇ   ‚îú‚îÄ‚îÄ GenVanillaNN.py
‚îÇ   ‚îî‚îÄ‚îÄ GenGAN.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ Dance/                      # poids entra√Æn√©s (.pth)
‚îÇ   ‚îú‚îÄ‚îÄ taichi1.mp4                 # vid√©o cible (dataset target)
‚îÇ   ‚îú‚îÄ‚îÄ taichi1.pkl                 # cache squelette/dataset
‚îÇ   ‚îú‚îÄ‚îÄ taichi1/                    # frames extraites/crop√©es
‚îÇ   ‚îú‚îÄ‚îÄ taichi2.mp4                 # exemple vid√©o source
‚îÇ   ‚îî‚îÄ‚îÄ karate1.mp4                 # autre exemple vid√©o source
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ environment.yml
‚îî‚îÄ‚îÄ README.md
```

### R√¥le de chaque fichier (src/)

**`DanceDemo.py`**  
Lance la d√©mo temps r√©el : lecture vid√©o source, extraction pose, appel au g√©n√©rateur choisi, affichage "SOURCE | SQUELETTE | GENERATION" + FPS + touches (`q`, `n`).

**`VideoSkeleton.py`**  
Construit le dataset cible √† partir d'une vid√©o (squelettes + images crop√©es), g√®re la mise en cache (`.pkl`) + sauvegarde frames sur disque, et fournit `cropAndSke` utilis√© en d√©mo.

**`VideoReader.py`**  
Lecture vid√©o (OpenCV), acc√®s total frames, lecture frame par frame et skip de N frames.

**`Skeleton.py`**  
Repr√©sentation du squelette : extraction MediaPipe Pose (33 landmarks), r√©duction √† 13 joints (26D), distance entre squelettes, bounding box, dessin du squelette (stickman).

**`Vec3.py`**  
Vecteurs 3D + op√©rations utiles (support interne pour les landmarks (x,y,z)).

**`GenNearest.py`**  
Baseline "Nearest Neighbor" : renvoie l'image target correspondant au squelette le plus proche.

**`GenVanillaNN.py`**  
Deux versions supervis√©es :
1. vecteur 26D ‚Üí image (d√©codeur par upsampling)
2. stickman image ‚Üí image (encoder-decoder / U-Net simplifi√© + am√©liorations)

**`GenGAN.py`**  
Raffinement GAN : g√©n√©rateur image‚Üíimage + critic/discriminator, entra√Ænement WGAN-GP + L1, sauvegarde/chargement de checkpoint.

---

## 2) Environnements (venv + conda)

Deux environnements ont √©t√© utilis√©s selon les membres du groupe : l'un via **venv/pip**, l'autre via **conda** (`environment.yml`).

### Option A ‚Äî venv (pip)

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

pip install -r requirements.txt
```

### Option B ‚Äî conda (environment.yml)

```bash
conda env create -f environment.yml
conda activate <ENV_NAME>
```

---

## 3) Ex√©cuter la d√©mo (projet principal)

### 3.1 Important : ex√©cuter depuis la racine du d√©p√¥t + PYTHONPATH

Le projet charge les mod√®les via des chemins relatifs (ex: `data/Dance/...`) et les imports Python sont √©crits sous la forme `from VideoSkeleton import ...`.

Il faut donc lancer depuis la racine et ajouter `src/` au `PYTHONPATH`.

**Linux / macOS**

```bash
PYTHONPATH=src python src/DanceDemo.py
```

**Windows (PowerShell)**

```powershell
$env:PYTHONPATH="src"
python src\DanceDemo.py
```

### 3.2 Ce que la d√©mo affiche

La fen√™tre de d√©mo affiche 3 panneaux :

- **SOURCE VIDEO** : frame source recadr√©e
- **SQUELETTE** : stickman (squelette dessin√©)
- **GENERATION** : image g√©n√©r√©e (target anim√© par la pose source)

**Contr√¥les clavier :**
- `q` : quitter
- `n` : sauter ~100 frames

### 3.3 Choisir le g√©n√©rateur (GEN_TYPE)

Dans `DanceDemo.py`, r√©gler `GEN_TYPE` :

| GEN_TYPE | M√©thode | R√©sum√© |
|:--------:|---------|--------|
| **1** | Nearest Neighbor | Baseline sans apprentissage. |
| **2** | Vanilla NN (26D) | Vecteur squelette 26D ‚Üí image. |
| **3** | Vanilla NN (stickman) | Image stickman ‚Üí image (meilleure structure). |
| **4** | GAN | WGAN-GP + L1 (am√©liore le r√©alisme). |

**Exemple** (changer la vid√©o source si besoin) :

```python
GEN_TYPE = 4
ddemo = DanceDemo("data/taichi2.mp4", GEN_TYPE)
ddemo.draw()
```

---

## 4) Aper√ßu du pipeline (donn√©es ‚Üí squelette ‚Üí g√©n√©ration)

### 4.1 Construction et mise en cache de l'ensemble de donn√©es cible (VideoSkeleton)

`VideoSkeleton` construit un dataset de paires (image target, squelette) √† partir de la vid√©o cible `taichi1.mp4`.

Pour √©viter de recalculer √† chaque ex√©cution, on met en cache :
- `data/taichi1.pkl` (m√©tadonn√©es + squelettes)
- `data/taichi1/` (frames crop√©es sauvegard√©es sur disque)

### 4.2 Extraction de squelette (MediaPipe Pose) et repr√©sentation

- **Extraction** par **MediaPipe Pose** : 33 landmarks (x, y, z).
- **Repr√©sentation r√©duite** utilis√©e pour l'apprentissage : 13 joints s√©lectionn√©s, uniquement (x, y) ‚Üí vecteur **26D**.
- **Repr√©sentation "image"** : squelette dessin√© sur une image (stickman), utilis√© pour les approches image‚Üíimage.

### 4.3 Recadrage et cas d'√©chec (robustesse de la d√©mo)

- Le recadrage est bas√© sur la bounding box du squelette (centre + cropRatio), avec padding si n√©cessaire.
- Si aucun squelette n'est d√©tect√©, la d√©mo affiche un panneau d'erreur (rouge) et skip la g√©n√©ration sur cette frame.
- Pour le temps r√©el, la d√©mo peut calculer une frame sur N (ex: 1/5) afin d'augmenter les FPS.

---

## 5) M√©thodes et concepts (√©tapes TP)

Le projet suit une progression p√©dagogique : **baseline ‚Üí apprentissage supervis√© ‚Üí image-to-image ‚Üí GAN**.

### 5.1 √âtape 1 ‚Äî Baseline Nearest Neighbor (GenNeirest)

**Id√©e :**  
Pour chaque pose source, chercher dans le dataset cible la pose la plus proche, puis renvoyer l'image correspondante (aucune g√©n√©ration).

**Distance :**  
Somme des distances joint-par-joint entre deux squelettes (utilise la repr√©sentation squelette du code).

**Limites :**  
Pas fluide, limit√© aux poses existantes, et co√ªt de recherche lin√©aire.

---

### 5.2 √âtape 2 ‚Äî Vanilla NN (vecteur 26D ‚Üí image) ‚Äî GenNNSke26ToImage

**Entr√©e :**  
Vecteur 26D = (x,y) de 13 articulations.

**Objectif :**  
Apprendre une fonction G : 26D ‚Üí image target 64√ó64.

**Concept d'architecture (d√©codeur) :**

- Projection (MLP) du 26D vers un tenseur compact (ex: 4√ó4√óC).
- Upsampling progressif par convolutions transpos√©es (ConvTranspose2d) pour atteindre 64√ó64.
- Normalisation (BatchNorm) + activations (souvent LeakyReLU) pour stabiliser.
- Sortie normalis√©e via `Tanh` ([-1,1]) puis d√©-normalisation pour affichage.

**Training (supervis√©) :**

Loss pixel-wise (MSE) : simple et stable, mais peut lisser les textures (effet "moyenne").

---

### 5.3 √âtape 3 ‚Äî Vanilla NN (image stickman ‚Üí image) ‚Äî GenNNSkeImToImage

**Entr√©e :**  
Image stickman (squelette dessin√©) 64√ó64.

**Objectif :**  
Apprendre G : stickman ‚Üí image target (image-to-image).

**Concept d'architecture (encoder-decoder / U-Net simplifi√©) :**

- **Encodeur** : convs qui compressent l'image pour extraire les features de pose.
- **D√©codeur** : convTranspose pour reconstruire l'image.
- **Skip connections** (type U-Net) : concat√©nation des features encodeur‚Üíd√©codeur pour conserver les d√©tails spatiaux.
- **Modules d'am√©lioration possibles** (dans votre version finale) : blocs r√©siduels + attention (self-attention) pour mieux mod√©liser la structure globale.

**Training :**

Toujours supervis√© (MSE/L1 possibles). L'image est plus structur√©e qu'un vecteur, donc souvent plus facile √† apprendre.

---

### 5.4 √âtape 4 ‚Äî Raffinement GAN ‚Äî GenGAN (WGAN-GP + L1)

**Motivation :**  
Les approches MSE ont tendance √† produire du flou. Un GAN force des textures plus r√©alistes.

**Principe :**

- **G√©n√©rateur G** : stickman ‚Üí image.
- **Discriminateur/Critic D** : distingue (r√©el vs g√©n√©r√©). PatchGAN possible : sortie en carte de patches.

**WGAN-GP + reconstruction :**

- **WGAN** : utilise un critic (pas de sigmoid) pour une optimisation plus stable.
- **Gradient Penalty (GP)** : stabilise l'entra√Ænement (√©vite l'explosion/instabilit√©).
- **Perte hybride** : adversarial (r√©alisme) + L1 (respect de la structure/pose).

---

## 6) Entra√Ænement (reproductibilit√©)

L'entra√Ænement utilise l'ensemble de donn√©es cible construit √† partir de `data/taichi1.mp4`.

### 6.1 Optionnel : reconstruire le cache

Si n√©cessaire, supprimez le cache et le r√©pertoire d'images puis r√©ex√©cutez l'entra√Ænement/d√©mo pour que `VideoSkeleton` les recalcule :

```bash
rm -f data/taichi1.pkl
rm -rf data/taichi1/
```

(Les utilisateurs Windows peuvent supprimer manuellement depuis l'Explorateur de fichiers.)

### 6.2 Entra√Æner VanillaNN (26D ou stickman)

Dans `GenVanillaNN.py`, configurez :

- `optSkeOrImage = 1` pour vecteur(26D)‚Üíimage
- `optSkeOrImage = 2` pour stickman‚Üíimage

et activez l'entra√Ænement dans la section `__main__`.

**Ex√©cutez :**

```bash
PYTHONPATH=src python src/GenVanillaNN.py data/taichi1.mp4
```

**Poids enregistr√©s** (selon le mode) :
- `data/Dance/DanceGenVanillaFromSke26.pth`
- `data/Dance/DanceGenVanillaFromSkeim.pth`

### 6.3 Entra√Æner GAN

Dans `GenGAN.py`, d√©finissez `train = True` et ex√©cutez :

```bash
PYTHONPATH=src python src/GenGAN.py data/taichi1.mp4
```

**Sortie :**
- `data/Dance/DanceGenGAN.pth`

---

## 7) Vid√©o de d√©monstration

Une vid√©o (~2 minutes) est fournie ici :
- `demo.mp4` (dans le ZIP / dans le d√©p√¥t) **ou** lien : `<PUT_LINK_HERE>`

Elle montre :
- L'ex√©cution de `DanceDemo.py` depuis la racine du d√©p√¥t (avec mod√®les entra√Æn√©s).
- Au moins deux modes (ex: `GEN_TYPE=1` baseline puis `GEN_TYPE=4` GAN).
- L'interface 3 panneaux, l'affichage FPS, et la sortie via `q`.

---

## 8) Bonus ‚Äî Application web Flask (ex√©cution uniquement)

**D√©p√¥t bonus :** https://github.com/infoelouarroudi-stack/DemoDaanceWEB

### 8.1 R√¥le (ce qu'elle ajoute)

Une petite interface web Flask a √©t√© ajout√©e en bonus pour ex√©cuter/visualiser la d√©mo via un navigateur, en s'appuyant sur :
- `app.py` (serveur Flask)
- `templates/index.html` (page d'accueil)
- `templates/viewer.html` (page de visualisation)
- `static/` (assets)

### 8.2 Installation & ex√©cution (venv)

Depuis la racine du d√©p√¥t Flask (l√† o√π se trouvent `app.py` et `requirements.txt`) :

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

pip install -r requirements.txt
python app.py
```

Puis ouvrir : **http://localhost:5000/**

---

## Limitations (observ√©es / attendues)

- **Coh√©rence temporelle limit√©e** (pas de contrainte explicite inter-frames), donc flickering possible.
- **R√©solution limit√©e** (64√ó64 au niveau des r√©seaux) : la qualit√© d√©pend fortement du dataset et du mod√®le.
- **La baseline Nearest Neighbor** est limit√©e au contenu exact de la vid√©o cible.

---

## Cr√©dits

- Inspir√© du concept **"Everybody Dance Now"** (Chan et al., ICCV 2019) dans le cadre du TP.
- Extraction de pose via **MediaPipe Pose** (starter code / pipeline du projet).
